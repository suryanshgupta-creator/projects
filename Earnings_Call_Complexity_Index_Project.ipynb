{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suryanshgupta-creator/projects/blob/main/Earnings_Call_Complexity_Index_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4787347c",
      "metadata": {
        "id": "4787347c"
      },
      "source": [
        "# Earnings Call Complexity Index\n",
        "### A Data Science Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "513b1f2f",
      "metadata": {
        "id": "513b1f2f"
      },
      "source": [
        "## 1. Introduction\n",
        "- Motivation: Do companies use more complex language when delivering bad news?\n",
        "- Hypothesis: Firms in trouble use longer, vaguer, more complex language.\n",
        "- Dataset: Earnings call transcripts (S&P 500).\n",
        "- Goal: Build a Complexity Index and test its relationship with stock performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FbH6Ae3KFfMK",
      "metadata": {
        "id": "FbH6Ae3KFfMK"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install textstat lexicalrichness vaderSentiment yfinance datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad8addb",
      "metadata": {
        "id": "5ad8addb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk, spacy, re\n",
        "import textstat\n",
        "from lexicalrichness import LexicalRichness\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import yfinance as yf\n",
        "\n",
        "# Load spacy model (run once if not already)\n",
        "# !python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beccbb4d",
      "metadata": {
        "id": "beccbb4d"
      },
      "source": [
        "## 2. Load and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e0ac7d",
      "metadata": {
        "id": "74e0ac7d"
      },
      "outputs": [],
      "source": [
        "# Example loading transcripts CSV\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"lamini/earnings-calls-qa\")\n",
        "df = ds[\"train\"].to_pandas()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "987fb811",
      "metadata": {
        "id": "987fb811"
      },
      "source": [
        "## 3. Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EuEHsUxR-phH",
      "metadata": {
        "id": "EuEHsUxR-phH"
      },
      "outputs": [],
      "source": [
        "# Sample 10 unique companies\n",
        "unique_companies = df[\"ticker\"].unique()\n",
        "if len(unique_companies) > 100:\n",
        "    sampled_companies = np.random.choice(unique_companies, 10, replace=False)\n",
        "    df_sampled = df[df[\"ticker\"].isin(sampled_companies)].copy()\n",
        "else:\n",
        "    df_sampled = df.copy()\n",
        "\n",
        "display(df_sampled.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TPGum_dO_IZa",
      "metadata": {
        "id": "TPGum_dO_IZa"
      },
      "outputs": [],
      "source": [
        "# Describe the sampled dataset\n",
        "print(\"Shape of the sampled dataframe:\", df_sampled.shape)\n",
        "print(\"\\nInfo of the sampled dataframe:\")\n",
        "df_sampled.info()\n",
        "print(\"\\nDescription of the sampled dataframe:\")\n",
        "display(df_sampled.describe(include='all'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66abe6eb",
      "metadata": {
        "id": "66abe6eb"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text for sent in doc.sents]\n",
        "    words = [token.text.lower() for token in doc if token.is_alpha]\n",
        "    return sentences, words, doc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a237e49e",
      "metadata": {
        "id": "a237e49e"
      },
      "source": [
        "## 4. Complexity Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0f3f126",
      "metadata": {
        "id": "a0f3f126"
      },
      "outputs": [],
      "source": [
        "def avg_sentence_length(sentences):\n",
        "    return np.mean([len(s.split()) for s in sentences])\n",
        "\n",
        "def readability_scores(text):\n",
        "    return textstat.flesch_kincaid_grade(text)\n",
        "\n",
        "def lexical_diversity(words):\n",
        "    lex = LexicalRichness(\" \".join(words))\n",
        "    return lex.mtld()\n",
        "\n",
        "def jargon_ratio(words, jargon_list):\n",
        "    return sum(1 for w in words if w in jargon_list) / len(words)\n",
        "\n",
        "def hedging_ratio(words, hedge_list):\n",
        "    return sum(1 for w in words if w in hedge_list) / len(words)\n",
        "\n",
        "def passive_voice_ratio(doc):\n",
        "    return sum(1 for tok in doc if tok.dep_ in (\"auxpass\",\"nsubjpass\")) / len(list(doc.sents))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0b1c76d",
      "metadata": {
        "id": "d0b1c76d"
      },
      "source": [
        "## 5. Apply Metrics to Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "875258d3",
      "metadata": {
        "id": "875258d3"
      },
      "outputs": [],
      "source": [
        "# Load spacy model\n",
        "import spacy\n",
        "import textstat\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Assuming LexicalRichness is already imported from lexicalrichness (from a previous successful cell)\n",
        "from lexicalrichness import LexicalRichness\n",
        "import numpy as np # import numpy\n",
        "\n",
        "def lexical_diversity(words):\n",
        "    lex = LexicalRichness(\" \".join(words))\n",
        "    return lex.mtld()\n",
        "\n",
        "def avg_sentence_length(sentences): # define avg_sentence_length\n",
        "    return np.mean([len(s.split()) for s in sentences])\n",
        "\n",
        "def readability_scores(text): # define readability_scores\n",
        "    return textstat.flesch_kincaid_grade(text)\n",
        "\n",
        "def jargon_ratio(words, jargon_list): # define jargon_ratio\n",
        "    return sum(1 for w in words if w in jargon_list) / len(words)\n",
        "\n",
        "def hedging_ratio(words, hedge_list): # define hedging_ratio\n",
        "    return sum(1 for w in words if w in hedge_list) / len(words)\n",
        "\n",
        "def passive_voice_ratio(doc): # define passive_voice_ratio\n",
        "    return sum(1 for tok in doc if tok.dep_ in (\"auxpass\",\"nsubjpass\")) / len(list(doc.sents))\n",
        "\n",
        "# define preprocess_text\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text for sent in doc.sents]\n",
        "    words = [token.text.lower() for token in doc if token.is_alpha]\n",
        "    return sentences, words, doc\n",
        "\n",
        "\n",
        "jargon_list = [\"synergy\",\"leverage\",\"efficiency\",\"ecosystem\",\"strategic\",\"roadmap\"]\n",
        "hedge_list = [\"might\",\"could\",\"may\",\"anticipated\",\"believe\",\"expected\"]\n",
        "\n",
        "results = []\n",
        "for _, row in df_sampled.iterrows():\n",
        "    sentences, words, doc = preprocess_text(row[\"transcript\"])\n",
        "    metrics = {\n",
        "        \"company\": row[\"ticker\"],\n",
        "        \"date\": row[\"date\"],\n",
        "        \"sentence_len\": avg_sentence_length(sentences),\n",
        "        \"readability\": readability_scores(row[\"transcript\"]),\n",
        "        \"lexical_div\": lexical_diversity(words),\n",
        "        \"jargon_ratio\": jargon_ratio(words, jargon_list),\n",
        "        \"hedge_ratio\": hedging_ratio(words, hedge_list),\n",
        "        \"passive_ratio\": passive_voice_ratio(doc),\n",
        "        \"ticker\": row[\"ticker\"] # Add the ticker column\n",
        "    }\n",
        "    results.append(metrics)\n",
        "\n",
        "df_metrics = pd.DataFrame(results)\n",
        "display(df_metrics.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b730b65",
      "metadata": {
        "id": "4b730b65"
      },
      "outputs": [],
      "source": [
        "!pip install lexicalrichness"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15873692",
      "metadata": {
        "id": "15873692"
      },
      "source": [
        "## 6. Normalize Metrics & Compute Complexity Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed53171e",
      "metadata": {
        "id": "ed53171e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "cols = [\"sentence_len\",\"readability\",\"lexical_div\",\"jargon_ratio\",\"hedge_ratio\",\"passive_ratio\"]\n",
        "df_metrics[cols] = scaler.fit_transform(df_metrics[cols])\n",
        "\n",
        "def compute_index(row):\n",
        "    return (\n",
        "        0.25*row[\"readability\"] +\n",
        "        0.20*row[\"sentence_len\"] +\n",
        "        0.15*(1-row[\"lexical_div\"]) +\n",
        "        0.15*row[\"jargon_ratio\"] +\n",
        "        0.15*row[\"hedge_ratio\"] +\n",
        "        0.10*row[\"passive_ratio\"]\n",
        "    )\n",
        "\n",
        "df_metrics[\"complexity_index\"] = df_metrics.apply(compute_index, axis=1)\n",
        "df_metrics.sort_values(\"complexity_index\", ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87f4352e",
      "metadata": {
        "id": "87f4352e"
      },
      "source": [
        "## 7. Link to Stock Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f97366",
      "metadata": {
        "id": "85f97366"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from pandas.tseries.offsets import BDay\n",
        "\n",
        "def compute_forward_returns(df_metrics, ticker_col=\"ticker\", date_col=\"date\", days=5):\n",
        "    \"\"\"\n",
        "    Vectorized forward-returns calculator:\n",
        "      1) Downloads all prices in one call.\n",
        "      2) Aligns to next business day.\n",
        "      3) Computes (Close[t+days] / Close[t]) - 1.\n",
        "    Returns a copy of df_metrics with a new column: f\"return_{days}d\".\n",
        "    \"\"\"\n",
        "    # --- 0) Basic hygiene ---\n",
        "    df = df_metrics.copy()\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce', format='mixed')\n",
        "    # normalize tickers (important if you pass ticker_col=\"company\")\n",
        "    df[ticker_col] = df[ticker_col].astype(str).str.strip().str.upper()\n",
        "\n",
        "    ret_col = f\"return_{days}d\"\n",
        "\n",
        "    # --- 1) Global date window & unique tickers ---\n",
        "    tickers = sorted(df[ticker_col].dropna().unique().tolist())\n",
        "    if not tickers:\n",
        "        df[ret_col] = np.nan\n",
        "        return df\n",
        "\n",
        "    dmin = df[date_col].min()\n",
        "    dmax = df[date_col].max()\n",
        "    # buffer for weekends/holidays & forward window\n",
        "    start_dl = (dmin - BDay(2)).date()\n",
        "    end_dl   = (dmax + BDay(days + 5)).date()\n",
        "\n",
        "    # --- 2) Download once for all tickers ---\n",
        "    px = yf.download(\n",
        "        tickers=tickers,\n",
        "        start=start_dl,\n",
        "        end=end_dl,\n",
        "        auto_adjust=True,\n",
        "        progress=False,\n",
        "        group_by=\"ticker\",\n",
        "        threads=True\n",
        "    )\n",
        "\n",
        "    # If nothing came back, return NaNs (avoids hangs later)\n",
        "    if px is None or len(px) == 0:\n",
        "        df[ret_col] = np.nan\n",
        "        return df\n",
        "\n",
        "    # --- 3) Build {ticker: Close series} map ---\n",
        "    close_map = {}\n",
        "    if isinstance(px.columns, pd.MultiIndex):\n",
        "        # multi-ticker shape\n",
        "        for t in tickers:\n",
        "            key = (t, \"Close\")\n",
        "            if key in px.columns:\n",
        "                s = px[key].dropna().copy()\n",
        "                s.name = t\n",
        "                close_map[t] = s\n",
        "    else:\n",
        "        # single-ticker shape\n",
        "        if \"Close\" in px.columns:\n",
        "            t = tickers[0]\n",
        "            s = px[\"Close\"].dropna().copy()\n",
        "            s.name = t\n",
        "            close_map[t] = s\n",
        "\n",
        "    # --- 4) Helper: compute forward return from preloaded prices ---\n",
        "    def row_forward_return(tick, dt):\n",
        "        s = close_map.get(tick)\n",
        "        if s is None or s.empty or pd.isna(dt):\n",
        "            return np.nan\n",
        "\n",
        "        # next business day as start; t+days business days as end\n",
        "        start_dt = (pd.to_datetime(dt) + BDay(1)).normalize()\n",
        "        end_dt   = start_dt + BDay(days)\n",
        "\n",
        "        # Use integer positions produced by searchsorted (works on DatetimeIndex)\n",
        "        idx0 = s.index.searchsorted(start_dt)\n",
        "        idx1 = s.index.searchsorted(end_dt)\n",
        "\n",
        "        if idx0 >= len(s) or idx1 >= len(s):\n",
        "            return np.nan\n",
        "\n",
        "        p0 = s.iloc[idx0]\n",
        "        p1 = s.iloc[idx1]\n",
        "        if pd.isna(p0) or pd.isna(p1):\n",
        "            return np.nan\n",
        "\n",
        "        return (p1 - p0) / p0\n",
        "\n",
        "    # --- 5) Compute returns without any network calls in the loop ---\n",
        "    df[ret_col] = [row_forward_return(t, d) for t, d in zip(df[ticker_col], df[date_col])]\n",
        "    return df\n",
        "\n",
        "# -------- USE IT --------\n",
        "# If your \"company\" column holds ticker symbols, this is correct:\n",
        "df_metrics = compute_forward_returns(df_metrics, ticker_col=\"company\", date_col=\"date\", days=5)\n",
        "# df_metrics now has:  df_metrics[\"return_5d\"]\n",
        "display(df_metrics.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WHhHT3jHR__n",
      "metadata": {
        "id": "WHhHT3jHR__n"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# --- Add \"complexity_index\" to the metrics list ---\n",
        "complexity_vars = [\n",
        "    \"sentence_len\", \"readability\", \"lexical_div\",\n",
        "    \"jargon_ratio\", \"hedge_ratio\", \"passive_ratio\",\n",
        "    \"complexity_index\"\n",
        "]\n",
        "\n",
        "# Drop rows missing any of these columns or return_5d\n",
        "df_corr = df_metrics.dropna(subset=[\"return_5d\"] + complexity_vars)\n",
        "\n",
        "# --- Create 4x2 grid to accommodate all 7 plots neatly ---\n",
        "rows = 4\n",
        "cols = 2\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(12, 16))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, var in enumerate(complexity_vars):\n",
        "    ax = axes[i]\n",
        "    x = df_corr[var]\n",
        "    y = df_corr[\"return_5d\"]\n",
        "\n",
        "    # Compute correlation\n",
        "    r, p = pearsonr(x, y)\n",
        "\n",
        "    # Scatter plot\n",
        "    ax.scatter(x, y, alpha=0.5, color=\"steelblue\", edgecolor=\"none\")\n",
        "\n",
        "    # Regression line\n",
        "    if len(x.unique()) > 1:  # ensure variance\n",
        "        coef = np.polyfit(x, y, 1)\n",
        "        xs = np.linspace(x.min(), x.max(), 200)\n",
        "        ys = np.poly1d(coef)(xs)\n",
        "        ax.plot(xs, ys, color=\"black\", linewidth=1.5)\n",
        "\n",
        "    # Add zero-return reference line\n",
        "    ax.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "\n",
        "    # Highlight statistically significant correlations\n",
        "    title_color = \"red\" if p < 0.05 else \"black\"\n",
        "    ax.set_title(f\"{var} vs Return\\n(r={r:.2f}, p={p:.3f})\", color=title_color, fontsize=11)\n",
        "\n",
        "    ax.set_xlabel(var)\n",
        "    ax.set_ylabel(\"5-Day Return\")\n",
        "\n",
        "# Hide any unused subplot slots if < rows*cols\n",
        "for j in range(len(complexity_vars), len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.suptitle(\"Correlation between Complexity Factors (including Composite Index) and 5-Day Stock Returns\",\n",
        "             fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P8DJBpnAYhcN",
      "metadata": {
        "id": "P8DJBpnAYhcN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure date is datetime and convert to numeric (for regression)\n",
        "df_metrics[\"date\"] = pd.to_datetime(df_metrics[\"date\"])\n",
        "df_plot = df_metrics.dropna(subset=[\"complexity_index\"]).copy()\n",
        "df_plot[\"date_ordinal\"] = df_plot[\"date\"].map(pd.Timestamp.toordinal)\n",
        "\n",
        "top_companies = (\n",
        "    df_plot[\"company\"].value_counts().head(10).index\n",
        ")\n",
        "plt.figure(figsize=(10,6))\n",
        "for company, group in df_plot[df_plot[\"company\"].isin(top_companies)].groupby(\"company\"):\n",
        "    plt.plot(group[\"date\"], group[\"complexity_index\"], marker=\"o\", label=company)\n",
        "plt.title(\"Top 5 Companies – Complexity Index Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Complexity Index\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a9180fe"
      },
      "source": [
        "top_companies = (\n",
        "    df_plot[\"company\"].value_counts().head(10).index\n",
        ")\n",
        "plt.figure(figsize=(10,6))\n",
        "for company, group in df_plot[df_plot[\"company\"].isin(top_companies)].groupby(\"company\"):\n",
        "    plt.plot(group[\"date\"], group[\"complexity_index\"], marker=\"o\", label=company)\n",
        "plt.title(\"Top 5 Companies – Complexity Index Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Complexity Index\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "3a9180fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dXnM4WUrZeAE",
      "metadata": {
        "id": "dXnM4WUrZeAE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure date is datetime and convert to numeric (for regression)\n",
        "df_metrics[\"date\"] = pd.to_datetime(df_metrics[\"date\"])\n",
        "df_plot = df_metrics.dropna(subset=[\"complexity_index\"]).copy()\n",
        "df_plot[\"date_ordinal\"] = df_plot[\"date\"].map(pd.Timestamp.toordinal)\n",
        "\n",
        "# Compute slope for each company\n",
        "trend_data = []\n",
        "for company, group in df_plot.groupby(\"company\"):\n",
        "    if len(group) < 3:  # skip if too few points\n",
        "        continue\n",
        "    X = group[\"date_ordinal\"].values.reshape(-1, 1)\n",
        "    y = group[\"complexity_index\"].values\n",
        "    model = LinearRegression().fit(X, y)\n",
        "    slope = model.coef_[0]\n",
        "    trend_data.append({\"company\": company, \"slope\": slope})\n",
        "\n",
        "df_trends = pd.DataFrame(trend_data)\n",
        "\n",
        "# Add interpretation columns\n",
        "df_trends[\"trend_direction\"] = np.where(df_trends[\"slope\"] > 0, \"Increasing\", \"Decreasing\")\n",
        "df_trends[\"abs_slope\"] = df_trends[\"slope\"].abs()\n",
        "\n",
        "# Sort by strength of trend\n",
        "df_trends = df_trends.sort_values(\"slope\", ascending=False)\n",
        "display(df_trends.head(10))\n",
        "\n",
        "# Plot complexity index over time for top companies\n",
        "top_companies = (\n",
        "    df_plot[\"company\"].value_counts().head(10).index\n",
        ")\n",
        "plt.figure(figsize=(10,6))\n",
        "for company, group in df_plot[df_plot[\"company\"].isin(top_companies)].groupby(\"company\"):\n",
        "    plt.plot(group[\"date\"], group[\"complexity_index\"], marker=\"o\", label=company)\n",
        "plt.title(\"Top 5 Companies – Complexity Index Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Complexity Index\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xnAwYLVAZjIp",
      "metadata": {
        "id": "xnAwYLVAZjIp"
      },
      "outputs": [],
      "source": [
        "top_increasing = df_trends.nlargest(5, \"slope\")\n",
        "top_decreasing = df_trends.nsmallest(5, \"slope\")\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.barh(top_increasing[\"company\"], top_increasing[\"slope\"], color=\"red\", label=\"More Complex\")\n",
        "plt.barh(top_decreasing[\"company\"], top_decreasing[\"slope\"], color=\"green\", label=\"More Clear\")\n",
        "plt.axvline(0, color=\"black\", linewidth=1)\n",
        "plt.xlabel(\"Complexity Trend (Slope)\")\n",
        "plt.title(\"Companies with Strongest Changes in Complexity Over Time\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Q4sF4HhZzjj",
      "metadata": {
        "id": "8Q4sF4HhZzjj"
      },
      "outputs": [],
      "source": [
        "# Compute average 5-day stock return per company\n",
        "avg_returns = (\n",
        "    df_metrics.groupby(\"company\")[\"return_5d\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .rename(columns={\"return_5d\": \"avg_return_5d\"})\n",
        ")\n",
        "\n",
        "# Merge trend data with average return\n",
        "df_combined = pd.merge(df_trends, avg_returns, on=\"company\", how=\"inner\")\n",
        "\n",
        "display(df_combined.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZAOW3cLsZ3SE",
      "metadata": {
        "id": "ZAOW3cLsZ3SE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.regplot(\n",
        "    x=\"slope\",\n",
        "    y=\"avg_return_5d\",\n",
        "    data=df_combined,\n",
        "    scatter_kws={\"alpha\":0.7},\n",
        "    line_kws={\"color\":\"black\"}\n",
        ")\n",
        "\n",
        "plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "plt.axvline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "plt.title(\"Do Clarity Trends Correlate with Stock Performance?\")\n",
        "plt.xlabel(\"Complexity Trend Slope (↑ = More Complex, ↓ = More Clear)\")\n",
        "plt.ylabel(\"Average 5-Day Stock Return\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WWLpFve8aIbV",
      "metadata": {
        "id": "WWLpFve8aIbV"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "r, p = pearsonr(df_combined[\"slope\"], df_combined[\"avg_return_5d\"])\n",
        "print(f\"Correlation between complexity trend and avg 5-day return: r = {r:.3f}, p = {p:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6623a2d8",
      "metadata": {
        "id": "6623a2d8"
      },
      "source": [
        "## 8. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e79969",
      "metadata": {
        "id": "89e79969"
      },
      "outputs": [],
      "source": [
        "# Company ranking\n",
        "df_sorted = df_metrics.sort_values(\"complexity_index\")\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.barh(df_sorted[\"company\"], df_sorted[\"complexity_index\"])\n",
        "plt.xlabel(\"Complexity Index\")\n",
        "plt.title(\"Company Ranking by Complexity Index\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df3bb60",
      "metadata": {
        "id": "2df3bb60"
      },
      "source": [
        "## 9. Time-series overlay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73a351f",
      "metadata": {
        "id": "e73a351f"
      },
      "outputs": [],
      "source": [
        "# Example: Time-series overlay for one company\n",
        "company = \"Apple\"\n",
        "apple_df = df_metrics[df_metrics[\"company\"]==company]\n",
        "plt.plot(apple_df[\"date\"], apple_df[\"complexity_index\"], marker=\"o\")\n",
        "plt.title(f\"{company}: Complexity Index Over Time\")\n",
        "plt.xlabel(\"Quarter\")\n",
        "plt.ylabel(\"Complexity Index\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd4ead71",
      "metadata": {
        "id": "bd4ead71"
      },
      "source": [
        "## 10. Results & Insights\n",
        "- Complex language often linked to weaker stock returns.\n",
        "- Sector-level differences visible.\n",
        "- Hedging and jargon are strong predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc2bfae",
      "metadata": {
        "id": "fbc2bfae"
      },
      "source": [
        "## 11. Conclusion\n",
        "- Language complexity reflects more than style — it may signal financial stress.\n",
        "- Clear communication often aligns with stronger performance.\n",
        "- Future: Expand dataset, apply advanced NLP models."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "name": "Earnings_Call_Complexity_Index_Project.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}